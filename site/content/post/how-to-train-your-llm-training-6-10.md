---
title: "How to train your LLM: Training (6/10)"
date: 2023-06-02T21:41:21.361Z
description: The advancement of training techniques has transformed the
  landscape of large language models, enhancing their performance and
  efficiency. By harnessing powerful GPU clustering capabilities, the training
  process has been accelerated, resulting in reduced training time and improved
  model quality. These advancements have greatly benefited models like
  Ghostwriter, pushing the boundaries of what is possible in natural language
  processing.
image: img/5e2a67abcd36412eb32a139de4ebdc64.png
---
# T﻿raining models on MosaicML

* G﻿PUs from multiple cloud providers
* W﻿ell-tuned LLM training configurations

  * L﻿earning rate, batch size, etc
* M﻿anaged infrastructure
* C﻿LI for kicking off training runs